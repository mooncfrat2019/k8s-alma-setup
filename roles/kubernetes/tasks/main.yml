---
- name: Установка и настройка containerd
  block:
    - name: Копирование rpm пакета containerd на сервер
      copy:
        src: "files/packages/containerd-2.1.4-alt1.x86_64.rpm"
        dest: /tmp/containerd.rpm
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Установка containerd через apt-get
      shell: apt-get install -y /tmp/containerd.rpm
      become: yes

    - name: Очистка временного файла
      file:
        path: /tmp/containerd.rpm
        state: absent
      become: yes

    - name: Создание базового конфига containerd
      copy:
        content: |
          version = 2
          root = "/var/lib/containerd"
          state = "/run/containerd"
          [grpc]
            address = "/run/containerd/containerd.sock"
          [plugins]
            [plugins."io.containerd.grpc.v1.cri"]
              sandbox_image = "{{ registry_url }}/pause:3.9"
              [plugins."io.containerd.grpc.v1.cri".registry]
                [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."89.208.208.139:5000"]
                    endpoint = ["http://89.208.208.139:5000"]
                [plugins."io.containerd.grpc.v1.cri".registry.configs]
                  [plugins."io.containerd.grpc.v1.cri".registry.configs."89.208.208.139:5000".tls]
                    insecure_skip_verify = true
              [plugins."io.containerd.grpc.v1.cri".containerd]
                snapshotter = "overlayfs"
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                  runtime_type = "io.containerd.runc.v2"
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                    SystemdCgroup = true
        dest: /etc/containerd/config.toml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Включение и запуск containerd
      systemd:
        name: containerd
        state: started
        enabled: yes
        daemon_reload: yes
      become: yes

    - name: Ожидание готовности containerd
      wait_for:
        path: /var/run/containerd/containerd.sock
        state: present
        timeout: 30
      become: yes

    - name: Проверка работы containerd
      command: crictl version
      register: containerd_check
      changed_when: false
      failed_when: containerd_check.rc != 0

- name: Установка и настройка crictl
  block:
    - name: Копирование и распаковка crictl
      unarchive:
        src: "files/binares/crictl-v1.28.0-linux-amd64.tar.gz"
        dest: /usr/bin
        owner: root
        group: root
        mode: '0755'
        remote_src: no
      become: yes

    - name: Настройка конфига crictl
      copy:
        content: |
          runtime-endpoint: unix:///var/run/containerd/containerd.sock
          image-endpoint: unix:///var/run/containerd/containerd.sock
          timeout: 10
          debug: false
        dest: /etc/crictl.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Проверка работы crictl
      command: crictl --version
      register: crictl_result
      changed_when: false

- name: Обновление apt cache
  shell: apt-get update
  become: yes

- name: Установка Kubernetes компонентов
  shell: |
    apt-get install -y \
      kubernetes1.28-client \
      kubernetes1.28-kubeadm \
      kubernetes1.28-kubelet \
      kubernetes1.28-master \
      kubernetes1.28-node \
      kubernetes1.28-common \
      kubernetes1.28-crio \
      python3-module-kubernetes-client \
      kube-vip
  become: yes

- name: Очистка предыдущей установки Kubernetes
  block:
    - name: Остановка kubelet
      systemd:
        name: kubelet
        state: stopped
      become: yes

    - name: Очистка данных containerd
      shell: |
        crictl rm -fa || true
        crictl rmi -a || true
      become: yes
      ignore_errors: yes

- name: Подготовка директорий на всех узлах
  block:
    - name: Создание необходимых директорий
      file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - /etc/kubernetes
        - /etc/kubernetes/pki
        - /etc/kubernetes/pki/etcd
      become: yes

- name: Создание конфигурации kubeadm для external etcd
  template:
    src: kubeadm-config-external-etcd.yaml.j2
    dest: /home/{{ k8s_user }}/kubeadm-config.yaml
    owner: "{{ k8s_user }}"
    group: "{{ k8s_group }}"
    mode: 0644
  when: inventory_hostname == "master-node-1"

- name: Генерация сертификатов Kubernetes на master-node-1
  block:
    - name: Генерация сертификатов
      command: kubeadm init phase certs all --config /home/{{ k8s_user }}/kubeadm-config.yaml
      when: inventory_hostname == "master-node-1"

    - name: Проверка сгенерированных сертификатов
      shell: |
        ls -la /etc/kubernetes/pki/
        ls -la /etc/kubernetes/pki/etcd/ || echo "No etcd certs"
      register: certs_check
      when: inventory_hostname == "master-node-1"

    - name: Отображение проверки сертификатов
      debug:
        var: certs_check.stdout
      when: inventory_hostname == "master-node-1"

- name: Копирование сертификатов с master-node-1 на хост Ansible
  block:
    - name: Создание временной директории на хосте Ansible
      tempfile:
        state: directory
        suffix: k8s_certs
      register: temp_certs_dir
      delegate_to: localhost
      run_once: true

    - name: Получение списка сертификатов с master-node-1
      find:
        paths: /etc/kubernetes/pki
        file_type: file
        patterns: '*'
        excludes: '*.yaml,*.yml,*.conf'
      register: cert_files
      delegate_to: master-node-1
      become: yes
      run_once: true

    - name: Скачивание сертификатов на хост Ansible
      fetch:
        src: "{{ item.path }}"
        dest: "{{ temp_certs_dir.path }}/{{ item.path | basename }}"
        flat: yes
      loop: "{{ cert_files.files }}"
      delegate_to: master-node-1
      become: yes
      run_once: true

    - name: Проверка скачанных сертификатов
      find:
        paths: "{{ temp_certs_dir.path }}"
        file_type: file
      register: downloaded_certs
      delegate_to: localhost
      run_once: true

    - name: Отображение скачанных сертификатов
      debug:
        var: downloaded_certs.files
      delegate_to: localhost
      run_once: true

- name: Копирование сертификатов с хоста Ansible на другие узлы
  block:
    - name: Копирование сертификатов на другие мастера
      copy:
        src: "{{ temp_certs_dir.path }}/{{ item }}"
        dest: "/etc/kubernetes/pki/{{ item }}"
        owner: root
        group: root
        mode: "{{ '0600' if item.endswith('.key') else '0644' }}"
      loop: "{{ downloaded_certs.files | map(attribute='path') | map('basename') | list }}"
      when: inventory_hostname != "master-node-1"

    - name: Проверка скопированных сертификатов
      shell: |
        echo "=== Сертификаты в /etc/kubernetes/pki/ ==="
        ls -la /etc/kubernetes/pki/
        echo "=== Сертификаты etcd ==="
        ls -la /etc/kubernetes/pki/etcd/ || echo "No etcd certs"
      register: certs_check
      become: yes
      when: inventory_hostname != "master-node-1"

    - name: Отображение проверки сертификатов
      debug:
        var: certs_check.stdout
      when: inventory_hostname != "master-node-1"

- name: Очистка временных файлов на хосте Ansible
  block:
    - name: Удаление временной директории
      file:
        path: "{{ temp_certs_dir.path }}"
        state: absent
      delegate_to: localhost
      run_once: true
  when: temp_certs_dir.path is defined

- name: Инициализация первого control-plane узла
  block:
    - name: Загрузка необходимых образов
      command: kubeadm config images pull --config /home/{{ k8s_user }}/kubeadm-config.yaml
      when: inventory_hostname == "master-node-1"

    - name: Инициализация кластера с external etcd
      command: |
        kubeadm init --config /home/{{ k8s_user }}/kubeadm-config.yaml --upload-certs -v=5
      register: kubeadm_init
      when: inventory_hostname == "master-node-1"

    - name: Отображение результата инициализации
      debug:
        var: kubeadm_init.stdout
      when: inventory_hostname == "master-node-1"

    - name: Извлечение join команд
      set_fact:
        certificate_key: "{{ kubeadm_init.stdout | regex_search('[a-f0-9]{64}') | first }}"
        control_plane_join_command: "{{ kubeadm_init.stdout_lines | select('match', 'kubeadm join.*--control-plane') | first | trim }}"
        worker_join_command: "{{ kubeadm_init.stdout_lines | select('match', 'kubeadm join.*--control-plane') | list | length == 0 | ternary(kubeadm_init.stdout_lines[-1], kubeadm_init.stdout_lines[-2]) | trim }}"
      when: inventory_hostname == "master-node-1"

    - name: Отображение join команд
      debug:
        msg: |
          Control Plane Join: {{ control_plane_join_command }}
          Worker Join: {{ worker_join_command }}
          Certificate Key: {{ certificate_key }}
      when: inventory_hostname == "master-node-1"

    - name: Настройка kubectl для пользователя
      block:
        - name: Создание .kube директории
          file:
            path: "{{ k8s_home }}/.kube"
            state: directory
            owner: "{{ k8s_user }}"
            group: "{{ k8s_group }}"
            mode: 0750

        - name: Копирование конфигурации
          copy:
            src: /etc/kubernetes/admin.conf
            dest: "{{ k8s_home }}/.kube/config"
            remote_src: yes
            owner: "{{ k8s_user }}"
            group: "{{ k8s_group }}"
            mode: 0600
      when: inventory_hostname == "master-node-1"

- name: Ожидание готовности первого control-plane узла
  block:
    - name: Ожидание запуска API server
      shell: |
        timeout 120 bash -c 'until curl -k https://127.0.0.1:6443/healthz &>/dev/null; do sleep 5; done'
      when: inventory_hostname == "master-node-1"

    - name: Проверка компонентов control plane
      command: kubectl get pods -n kube-system -l tier=control-plane
      register: control_plane_pods
      when: inventory_hostname == "master-node-1"

    - name: Отображение статуса control plane
      debug:
        var: control_plane_pods.stdout
      when: inventory_hostname == "master-node-1"

- name: Установка CNI плагина (Flannel)
  block:
    - name: Применение Flannel
      command: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      when: inventory_hostname == "master-node-1"

    - name: Ожидание запуска Flannel
      command: kubectl wait --for=condition=ready pod -l app=flannel -n kube-flannel --timeout=300s
      when: inventory_hostname == "master-node-1"

- name: Присоединение остальных control-plane узлов
  block:
    - name: Присоединение control-plane узла
      command: "{{ control_plane_join_command }}"
      when:
        - inventory_hostname != "master-node-1"
        - inventory_hostname in groups['masters']

    - name: Настройка kubectl на других control-plane узлах
      block:
        - name: Создание .kube директории
          file:
            path: "{{ k8s_home }}/.kube"
            state: directory
            owner: "{{ k8s_user }}"
            group: "{{ k8s_group }}"
            mode: 0750

        - name: Копирование конфигурации с первого узла
          slurp:
            src: "{{ k8s_home }}/.kube/config"
          register: kubeconfig
          delegate_to: master-node-1

        - name: Запись конфигурации
          copy:
            content: "{{ kubeconfig.content | b64decode }}"
            dest: "{{ k8s_home }}/.kube/config"
            owner: "{{ k8s_user }}"
            group: "{{ k8s_group }}"
            mode: 0600
      when:
        - inventory_hostname != "master-node-1"
        - inventory_hostname in groups['masters']

- name: Проверка состояния кластера
  block:
    - name: Проверка узлов
      command: kubectl get nodes -o wide
      register: nodes_info
      when: inventory_hostname == "master-node-1"

    - name: Отображение информации об узлах
      debug:
        var: nodes_info.stdout
      when: inventory_hostname == "master-node-1"

    - name: Проверка подключения к etcd
      shell: |
        ETCDCTL_API=3 etcdctl \
          --endpoints=https://{{ hostvars['master-node-1'].internal_ip }}:2379,https://{{ hostvars['master-node-2'].internal_ip }}:2379,https://{{ hostvars['master-node-3'].internal_ip }}:2379 \
          --cacert=/etc/kubernetes/pki/etcd/etcd-ca.crt \
          --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt \
          --key=/etc/kubernetes/pki/apiserver-etcd-client.key \
          endpoint health
      register: etcd_health
      when: inventory_hostname == "master-node-1"

    - name: Отображение статуса etcd
      debug:
        var: etcd_health.stdout
      when: inventory_hostname == "master-node-1"