---
- name: Установка и настройка containerd
  block:
    - name: Копирование rpm пакета containerd на сервер
      copy:
        src: "files/packages/containerd-2.1.4-alt1.x86_64.rpm"
        dest: /tmp/containerd.rpm
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Установка containerd через apt-get
      shell: apt-get install -y /tmp/containerd.rpm
      become: yes

    - name: Очистка временного файла
      file:
        path: /tmp/containerd.rpm
        state: absent
      become: yes

    - name: Создание базового конфига containerd
      copy:
        content: |
          version = 2
          root = "/var/lib/containerd"
          state = "/run/containerd"
          [grpc]
            address = "/run/containerd/containerd.sock"
          [plugins]
            [plugins."io.containerd.grpc.v1.cri"]
              sandbox_image = "{{ registry_url }}/pause:3.9"
              [plugins."io.containerd.grpc.v1.cri".registry]
                [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."89.208.208.139:5000"]
                    endpoint = ["http://89.208.208.139:5000"]
                [plugins."io.containerd.grpc.v1.cri".registry.configs]
                  [plugins."io.containerd.grpc.v1.cri".registry.configs."89.208.208.139:5000".tls]
                    insecure_skip_verify = true
              [plugins."io.containerd.grpc.v1.cri".containerd]
                snapshotter = "overlayfs"
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                  runtime_type = "io.containerd.runc.v2"
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                    SystemdCgroup = true
        dest: /etc/containerd/config.toml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Включение и запуск containerd
      systemd:
        name: containerd
        state: started
        enabled: yes
        daemon_reload: yes
      become: yes

    - name: Ожидание готовности containerd
      wait_for:
        path: /var/run/containerd/containerd.sock
        state: present
        timeout: 30
      become: yes

    - name: Проверка работы containerd
      command: crictl version
      register: containerd_check
      changed_when: false
      failed_when: containerd_check.rc != 0

- name: Установка и настройка crictl
  block:
    - name: Копирование и распаковка crictl
      unarchive:
        src: "files/binares/crictl-v1.28.0-linux-amd64.tar.gz"
        dest: /usr/bin
        owner: root
        group: root
        mode: '0755'
        remote_src: no
      become: yes

    - name: Настройка конфига crictl
      copy:
        content: |
          runtime-endpoint: unix:///var/run/containerd/containerd.sock
          image-endpoint: unix:///var/run/containerd/containerd.sock
          timeout: 10
          debug: false
        dest: /etc/crictl.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Проверка работы crictl
      command: crictl --version
      register: crictl_result
      changed_when: false

- name: Обновление apt cache
  shell: apt-get update
  become: yes

- name: Установка Kubernetes компонентов
  shell: |
    apt-get install -y \
      kubernetes1.28-client \
      kubernetes1.28-kubeadm \
      kubernetes1.28-kubelet \
      kubernetes1.28-master \
      kubernetes1.28-node \
      kubernetes1.28-common \
      kubernetes1.28-crio \
      python3-module-kubernetes-client \
      kube-vip
  become: yes

- name: Подготовка директорий для Kubernetes
  block:
    - name: Остановка kubelet если запущен
      systemd:
        name: kubelet
        state: stopped
      become: yes
      ignore_errors: yes

    - name: Создание необходимых директорий
      file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - /etc/kubernetes
        - /var/lib/etcd
        - /var/lib/kubelet
        - /etc/kubernetes/manifests
        - /etc/kubernetes/pki
      become: yes
  when: inventory_hostname == "master-node-1"

- name: Создание конфигурации kubeadm для первого мастера
  template:
    src: kubeadm-config.yaml.j2
    dest: "/home/{{ k8s_user }}/kubeadm-config.yaml"
    owner: "{{ k8s_user }}"
    group: "{{ k8s_group }}"
    mode: 0644
  when: inventory_hostname == "master-node-1"

- name: Генерация сертификатов Kubernetes
  command: |
    kubeadm init phase certs all --config /home/{{ k8s_user }}/kubeadm-config.yaml
  when: inventory_hostname == "master-node-1"

- name: Создание kubeconfig файлов
  command: |
    kubeadm init phase kubeconfig all --config /home/{{ k8s_user }}/kubeadm-config.yaml
  when: inventory_hostname == "master-node-1"

- name: Создание ВСЕХ манифестов static pods перед запуском kubelet
  block:
    - name: Создание манифеста etcd
      copy:
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            annotations:
              kubeadm.kubernetes.io/etcd.advertise-client-urls: https://{{ master_ips[inventory_hostname] }}:2379
            creationTimestamp: null
            labels:
              component: etcd
              tier: control-plane
            name: etcd
            namespace: kube-system
          spec:
            containers:
            - command:
              - etcd
              - --advertise-client-urls=https://{{ master_ips[inventory_hostname] }}:2379
              - --cert-file=/etc/kubernetes/pki/etcd/server.crt
              - --client-cert-auth=true
              - --data-dir=/var/lib/etcd
              - --initial-advertise-peer-urls=https://{{ master_ips[inventory_hostname] }}:2380
              - --initial-cluster=master-node-1=https://{{ master_ips[inventory_hostname] }}:2380
              - --key-file=/etc/kubernetes/pki/etcd/server.key
              - --listen-client-urls=https://0.0.0.0:2379
              - --listen-metrics-urls=http://0.0.0.0:2381
              - --listen-peer-urls=https://0.0.0.0:2380
              - --name=master-node-1
              - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
              - --peer-client-cert-auth=true
              - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
              - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
              - --snapshot-count=10000
              - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
              image: {{ registry_url }}/etcd:3.5.15-0
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 8
                httpGet:
                  host: 127.0.0.1
                  path: /health
                  port: 2381
                  scheme: HTTP
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              name: etcd
              resources: {}
              startupProbe:
                failureThreshold: 24
                httpGet:
                  host: 127.0.0.1
                  path: /health
                  port: 2381
                  scheme: HTTP
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /var/lib/etcd
                name: etcd-data
              - mountPath: /etc/kubernetes/pki/etcd
                name: etcd-certs
            hostNetwork: true
            priorityClassName: system-node-critical
            volumes:
            - hostPath:
                path: /var/lib/etcd
                type: DirectoryOrCreate
              name: etcd-data
            - hostPath:
                path: /etc/kubernetes/pki/etcd
                type: DirectoryOrCreate
              name: etcd-certs
        dest: /etc/kubernetes/manifests/etcd.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Создание манифеста kube-apiserver
      copy:
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            labels:
              component: kube-apiserver
              tier: control-plane
            name: kube-apiserver
            namespace: kube-system
          spec:
            containers:
            - command:
              - kube-apiserver
              - --advertise-address={{ master_ips[inventory_hostname] }}
              - --allow-privileged=true
              - --authorization-mode=Node,RBAC
              - --client-ca-file=/etc/kubernetes/pki/ca.crt
              - --enable-admission-plugins=NodeRestriction
              - --enable-bootstrap-token-auth=true
              - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
              - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
              - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
              - --etcd-servers=https://127.0.0.1:2379
              - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
              - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
              - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
              - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
              - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
              - --requestheader-allowed-names=front-proxy-client
              - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
              - --requestheader-extra-headers-prefix=X-Remote-Extra-
              - --requestheader-group-headers=X-Remote-Group
              - --requestheader-username-headers=X-Remote-User
              - --secure-port=6443
              - --service-account-issuer=https://kubernetes.default.svc.cluster.local
              - --service-account-key-file=/etc/kubernetes/pki/sa.pub
              - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
              - --service-cluster-ip-range={{ service_cidr }}
              - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
              - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
              image: {{ registry_url }}/kube-apiserver:v1.28.15
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 8
                httpGet:
                  host: 127.0.0.1
                  path: /livez
                  port: 6443
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              name: kube-apiserver
              resources:
                requests:
                  cpu: 250m
              startupProbe:
                failureThreshold: 24
                httpGet:
                  host: 127.0.0.1
                  path: /livez
                  port: 6443
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /etc/ssl/certs
                name: ca-certs
                readOnly: true
              - mountPath: /etc/ca-certificates
                name: etc-pki
                readOnly: true
              - mountPath: /etc/kubernetes/pki
                name: k8s-certs
                readOnly: true
              - mountPath: /usr/share/ca-certificates
                name: usr-share-ca-certificates
                readOnly: true
            hostNetwork: true
            priorityClassName: system-node-critical
            volumes:
            - hostPath:
                path: /etc/ssl/certs
                type: DirectoryOrCreate
              name: ca-certs
            - hostPath:
                path: /etc/pki
                type: DirectoryOrCreate
              name: etc-pki
            - hostPath:
                path: /etc/kubernetes/pki
                type: DirectoryOrCreate
              name: k8s-certs
            - hostPath:
                path: /usr/share/ca-certificates
                type: DirectoryOrCreate
              name: usr-share-ca-certificates
        dest: /etc/kubernetes/manifests/kube-apiserver.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Создание манифеста kube-controller-manager
      copy:
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            labels:
              component: kube-controller-manager
              tier: control-plane
            name: kube-controller-manager
            namespace: kube-system
          spec:
            containers:
            - command:
              - kube-controller-manager
              - --allocate-node-cidrs=true
              - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
              - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
              - --bind-address=127.0.0.1
              - --client-ca-file=/etc/kubernetes/pki/ca.crt
              - --cluster-cidr={{ pod_network_cidr }}
              - --cluster-name=kubernetes
              - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
              - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
              - --controllers=*,bootstrapsigner,tokencleaner
              - --kubeconfig=/etc/kubernetes/controller-manager.conf
              - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
              - --root-ca-file=/etc/kubernetes/pki/ca.crt
              - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
              - --use-service-account-credentials=true
              image: {{ registry_url }}/kube-controller-manager:v1.28.15
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 8
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10257
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              name: kube-controller-manager
              resources:
                requests:
                  cpu: 200m
              startupProbe:
                failureThreshold: 24
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10257
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /etc/ssl/certs
                name: ca-certs
                readOnly: true
              - mountPath: /etc/ca-certificates
                name: etc-pki
                readOnly: true
              - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
                name: flexvolume-dir
              - mountPath: /etc/kubernetes/pki
                name: k8s-certs
                readOnly: true
              - mountPath: /etc/kubernetes/controller-manager.conf
                name: kubeconfig
                readOnly: true
              - mountPath: /usr/share/ca-certificates
                name: usr-share-ca-certificates
                readOnly: true
            hostNetwork: true
            priorityClassName: system-node-critical
            volumes:
            - hostPath:
                path: /etc/ssl/certs
                type: DirectoryOrCreate
              name: ca-certs
            - hostPath:
                path: /etc/pki
                type: DirectoryOrCreate
              name: etc-pki
            - hostPath:
                path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
                type: DirectoryOrCreate
              name: flexvolume-dir
            - hostPath:
                path: /etc/kubernetes/pki
                type: DirectoryOrCreate
              name: k8s-certs
            - hostPath:
                path: /etc/kubernetes/controller-manager.conf
                type: FileOrCreate
              name: kubeconfig
            - hostPath:
                path: /usr/share/ca-certificates
                type: DirectoryOrCreate
              name: usr-share-ca-certificates
        dest: /etc/kubernetes/manifests/kube-controller-manager.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Создание манифеста kube-scheduler
      copy:
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            labels:
              component: kube-scheduler
              tier: control-plane
            name: kube-scheduler
            namespace: kube-system
          spec:
            containers:
            - command:
              - kube-scheduler
              - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
              - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
              - --bind-address=127.0.0.1
              - --kubeconfig=/etc/kubernetes/scheduler.conf
              image: {{ registry_url }}/kube-scheduler:v1.28.15
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 8
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10259
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              name: kube-scheduler
              resources:
                requests:
                  cpu: 100m
              startupProbe:
                failureThreshold: 24
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10259
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /etc/kubernetes/scheduler.conf
                name: kubeconfig
                readOnly: true
            hostNetwork: true
            priorityClassName: system-node-critical
            volumes:
            - hostPath:
                path: /etc/kubernetes/scheduler.conf
                type: FileOrCreate
              name: kubeconfig
        dest: /etc/kubernetes/manifests/kube-scheduler.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes
  when: inventory_hostname == "master-node-1"

- name: Создание systemd сервиса для kubelet
  copy:
    content: |
      [Unit]
      Description=Kubernetes Kubelet
      Documentation=https://kubernetes.io/docs/home/
      Wants=network-online.target
      After=network-online.target
      After=containerd.service

      [Service]
      ExecStart=/usr/bin/kubelet \
        --config=/var/lib/kubelet/config.yaml \
        --kubeconfig=/etc/kubernetes/kubelet.conf \
        --pod-infra-container-image={{ registry_url }}/pause:3.9 \
        --fail-swap-on=false \
        --v=2
      Restart=always
      StartLimitInterval=0
      RestartSec=10

      [Install]
      WantedBy=multi-user.target
    dest: /lib/systemd/system/kubelet.service
    owner: root
    group: root
    mode: '0644'
  become: yes

- name: Создание базовой конфигурации kubelet
  copy:
    content: |
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      cgroupDriver: systemd
      clusterDomain: cluster.local
      clusterDNS:
      - 10.96.0.10
      staticPodPath: /etc/kubernetes/manifests
      failSwapOn: false
    dest: /var/lib/kubelet/config.yaml
    owner: root
    group: root
    mode: '0644'
  become: yes

- name: Перезагрузка демона systemd
  systemd:
    daemon_reload: yes
  become: yes

- name: Запуск kubelet
  systemd:
    name: kubelet
    state: started
    enabled: yes
  become: yes

- name: Проверка здоровья Kubernetes API
  shell: |
    curl -k -s -o /dev/null -w "%{http_code}" https://127.0.0.1:6443/healthz
  register: api_health_check
  failed_when: api_health_check.stdout != "200"
  retries: 30
  delay: 10
  when: inventory_hostname == "master-node-1"

- name: Отображение статуса проверки API
  debug:
    msg: "Kubernetes API health check: {{ api_health_check.stdout }}"
  when: inventory_hostname == "master-node-1"

- name: Копирование конфигурации для пользователя
  copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ k8s_home }}/.kube/config"
    remote_src: yes
    owner: "{{ k8s_user }}"
    group: "{{ k8s_group }}"
    mode: 0600
  when: inventory_hostname == "master-node-1"

- name: Установка Calico CNI плагина
  block:
    - name: Скачивание манифеста Calico
      get_url:
        url: https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/calico.yaml
        dest: /tmp/calico.yaml
        mode: '0644'

    - name: Применение манифеста Calico
      command: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /tmp/calico.yaml
      register: calico_apply
      changed_when: calico_apply.rc == 0

    - name: Ожидание запуска подов Calico
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l k8s-app=calico-node --no-headers
      register: calico_pods
      until: calico_pods.stdout != ""
      retries: 30
      delay: 10
      when: inventory_hostname == "master-node-1"

    - name: Проверка статуса Calico подов
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l k8s-app=calico-node -o wide
      register: calico_status
      when: inventory_hostname == "master-node-1"

    - name: Отображение статуса Calico
      debug:
        var: calico_status.stdout
      when: inventory_hostname == "master-node-1"
  when: inventory_hostname == "master-node-1"

- name: Попытка снятия taint с control-plane узлов
  command: |
    kubectl --kubeconfig=/etc/kubernetes/admin.conf taint nodes --all node-role.kubernetes.io/control-plane- --overwrite
  register: taint_result
  failed_when: false
  changed_when: false
  when: inventory_hostname == "master-node-1"

- name: Отображение результата taint
  debug:
    msg: "Taint result: {{ taint_result.stdout }}{{ taint_result.stderr }}"
  when: inventory_hostname == "master-node-1"

- name: Принудительное создание cluster-info kubeadm
  command: |
    kubeadm init phase bootstrap-token --kubeconfig=/etc/kubernetes/admin.conf
  when: inventory_hostname == "master-node-1"

- name: Настройка RBAC для разрешения анонимного доступа к cluster-info
  command: |
    kubectl --kubeconfig=/etc/kubernetes/admin.conf create clusterrolebinding kubeadm-node-clusterinfo --clusterrole=cluster-info-reader --group=system:unauthenticated --group=system:authenticated
  when: inventory_hostname == "master-node-1"

- name: Проверка доступа к cluster-info
  command: |
    kubectl --kubeconfig=/etc/kubernetes/admin.conf get configmap cluster-info -n kube-public
  when: inventory_hostname == "master-node-1"

- name: Получение команды join для control-plane
  command: kubeadm token create --print-join-command
  register: join_command
  changed_when: false
  when: inventory_hostname == "master-node-1"

- name: Отображение полученной join команды
  debug:
    var: join_command.stdout
  when: inventory_hostname == "master-node-1"

- name: Получение certificate key с sudo
  shell: kubeadm init phase upload-certs --upload-certs
  register: cert_key
  changed_when: false
  become: true
  when: inventory_hostname == "master-node-1"

- name: Отображение полученного certificate key
  debug:
    var: cert_key.stdout
  when: inventory_hostname == "master-node-1"

- name: Извлечение certificate key из вывода
  set_fact:
    certificate_key: "{{ cert_key.stdout | regex_search('[a-f0-9]{64}') | first }}"
  when: inventory_hostname == "master-node-1"

- name: Отображение извлеченного certificate key
  debug:
    var: certificate_key
  when: inventory_hostname == "master-node-1"

- name: Сохранение команд join в переменные
  set_fact:
    control_plane_join_command: "kubeadm join {{ cluster_vip }}:6443 --token sujx2i.y16zxicq1bbmfgs4 --discovery-token-ca-cert-hash sha256:069317430606299c62e5a0afa1013fc3136d5c6b11ef8bb223f8987da0418b2d --control-plane --certificate-key {{ certificate_key }}"
    worker_join_command: "kubeadm join {{ cluster_vip }}:6443 --token sujx2i.y16zxicq1bbmfgs4 --discovery-token-ca-cert-hash sha256:069317430606299c62e5a0afa1013fc3136d5c6b11ef8bb223f8987da0418b2d"
  when: inventory_hostname == "master-node-1"

- name: Присоединение других мастер-узлов
  command: "{{ hostvars['master-node-1']['control_plane_join_command'] }}"
  when:
    - inventory_hostname != "master-node-1"
    - inventory_hostname in groups['masters']

- name: Настройка kubectl на других мастерах
  block:
    - name: Копирование конфигурации с первого мастера
      slurp:
        src: "{{ k8s_home }}/.kube/config"
      register: kubeconfig
      delegate_to: master-node-1

    - name: Запись конфигурации на текущий узел
      copy:
        content: "{{ kubeconfig.content | b64decode }}"
        dest: "{{ k8s_home }}/.kube/config"
        owner: "{{ k8s_user }}"
        group: "{{ k8s_group }}"
        mode: 0600
  when:
    - inventory_hostname != "master-node-1"
    - inventory_hostname in groups['masters']