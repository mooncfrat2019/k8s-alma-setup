---
- name: Установка и настройка containerd
  block:
    - name: Копирование rpm пакета containerd на сервер
      copy:
        src: "files/packages/containerd-2.1.4-alt1.x86_64.rpm"
        dest: /tmp/containerd.rpm
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Установка containerd через apt-get
      shell: apt-get install -y /tmp/containerd.rpm
      become: yes

    - name: Очистка временного файла
      file:
        path: /tmp/containerd.rpm
        state: absent
      become: yes

    - name: Создание базового конфига containerd
      copy:
        content: |
          version = 2
          root = "/var/lib/containerd"
          state = "/run/containerd"
          [grpc]
            address = "/run/containerd/containerd.sock"
          [plugins]
            [plugins."io.containerd.grpc.v1.cri"]
              sandbox_image = "{{ registry_url }}/pause:3.9"
              [plugins."io.containerd.grpc.v1.cri".registry]
                [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."89.208.208.139:5000"]
                    endpoint = ["http://89.208.208.139:5000"]
                [plugins."io.containerd.grpc.v1.cri".registry.configs]
                  [plugins."io.containerd.grpc.v1.cri".registry.configs."89.208.208.139:5000".tls]
                    insecure_skip_verify = true
              [plugins."io.containerd.grpc.v1.cri".containerd]
                snapshotter = "overlayfs"
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                  runtime_type = "io.containerd.runc.v2"
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                    SystemdCgroup = true
        dest: /etc/containerd/config.toml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Включение и запуск containerd
      systemd:
        name: containerd
        state: started
        enabled: yes
        daemon_reload: yes
      become: yes

    - name: Ожидание готовности containerd
      wait_for:
        path: /var/run/containerd/containerd.sock
        state: present
        timeout: 30
      become: yes

    - name: Проверка работы containerd
      command: crictl version
      register: containerd_check
      changed_when: false
      failed_when: containerd_check.rc != 0
  when: need_k8s_prepare | default(true) | bool

- name: Установка и настройка crictl
  block:
    - name: Копирование и распаковка crictl
      unarchive:
        src: "files/binares/crictl-v1.28.0-linux-amd64.tar.gz"
        dest: /usr/bin
        owner: root
        group: root
        mode: '0755'
        remote_src: no
      become: yes

    - name: Настройка конфига crictl
      copy:
        content: |
          runtime-endpoint: unix:///var/run/containerd/containerd.sock
          image-endpoint: unix:///var/run/containerd/containerd.sock
          timeout: 10
          debug: false
        dest: /etc/crictl.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Проверка работы crictl
      command: crictl --version
      register: crictl_result
      changed_when: false
  when: need_k8s_prepare | default(true) | bool

- name: Обновление apt cache
  shell: apt-get update
  become: yes

- name: Установка Kubernetes компонентов
  shell: |
    apt-get install -y \
      kubernetes1.28-client \
      kubernetes1.28-kubeadm \
      kubernetes1.28-kubelet \
      kubernetes1.28-master \
      kubernetes1.28-node \
      kubernetes1.28-common \
      kubernetes1.28-crio \
      python3-module-kubernetes-client \
      kube-vip
  become: yes
  when: need_k8s_prepare | default(true) | bool

- name: Очистка предыдущей установки Kubernetes
  block:
    - name: Остановка kubelet
      systemd:
        name: kubelet
        state: stopped
      become: yes

    - name: Очистка данных containerd
      shell: |
        crictl rm -fa || true
        crictl rmi -a || true
      become: yes
      ignore_errors: yes
  when: need_k8s_prepare | default(true) | bool

- name: Подготовка директорий на всех узлах
  block:
    - name: Создание необходимых директорий
      file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - /etc/kubernetes
        - /etc/kubernetes/pki
        - /etc/kubernetes/pki/etcd
      become: yes
  when: need_k8s_prepare | default(true) | bool

- name: Создание конфигурации kubeadm для external etcd
  template:
    src: kubeadm-config-external-etcd.yaml.j2
    dest: /home/{{ k8s_user }}/kubeadm-config.yaml
    owner: "{{ k8s_user }}"
    group: "{{ k8s_group }}"
    mode: 0644

- name: Принудительная перегенерация всех сертификатов
  block:
    - name: Проверка текущих SAN в сертификате (если существует)
      shell: |
        if [ -f /etc/kubernetes/pki/apiserver.crt ]; then
          openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep -A20 "X509v3 Subject Alternative Name"
        else
          echo "Certificate does not exist yet"
        fi
      register: current_san_check
      when: inventory_hostname == "master-node-1"

    - name: Отображение текущих SAN
      debug:
        var: current_san_check.stdout
      when: inventory_hostname == "master-node-1"

    - name: Удаление ВСЕХ существующих сертификатов
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes/pki/apiserver.crt
        - /etc/kubernetes/pki/apiserver.key
        - /etc/kubernetes/pki/apiserver-kubelet-client.crt
        - /etc/kubernetes/pki/apiserver-kubelet-client.key
        - /etc/kubernetes/pki/front-proxy-client.crt
        - /etc/kubernetes/pki/front-proxy-client.key
        - /etc/kubernetes/pki/ca.crt
        - /etc/kubernetes/pki/ca.key
        - /etc/kubernetes/pki/sa.key
        - /etc/kubernetes/pki/sa.pub
        - /etc/kubernetes/pki/front-proxy-ca.crt
        - /etc/kubernetes/pki/front-proxy-ca.key
      become: yes
      ignore_errors: yes
      when: inventory_hostname == "master-node-1"

    - name: Создание чистых директорий после удаления
      file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - /etc/kubernetes/pki
        - /etc/kubernetes/pki/etcd
      become: yes
      when: inventory_hostname == "master-node-1"

    - name: Генерация всех Kubernetes сертификатов заново
      command: kubeadm init phase certs all --config /home/{{ k8s_user }}/kubeadm-config.yaml
      when: inventory_hostname == "master-node-1"

    - name: Проверка новых SAN в сертификате
      shell: |
        openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep -A20 "X509v3 Subject Alternative Name"
      register: new_san_check
      when: inventory_hostname == "master-node-1"

    - name: Отображение новых SAN
      debug:
        var: new_san_check.stdout
      when: inventory_hostname == "master-node-1"

    - name: Проверка наличия kubernetes.default.svc.k8s-ha.local в SAN
      shell: |
        openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep "kubernetes.default.svc.k8s-ha.local"
      register: k8s_domain_check
      when: inventory_hostname == "master-node-1"

    - name: Отображение проверки домена
      debug:
        var: k8s_domain_check.stdout
      when: inventory_hostname == "master-node-1"
  when: inventory_hostname == "master-node-1"

#- name: Генерация сертификатов Kubernetes на master-node-1
#  block:
#    - name: Генерация сертификатов
#      command: kubeadm init phase certs all --config /home/{{ k8s_user }}/kubeadm-config.yaml
#      when: inventory_hostname == "master-node-1"
#
#    - name: Проверка сгенерированных сертификатов
#      shell: |
#        ls -la /etc/kubernetes/pki/
#        ls -la /etc/kubernetes/pki/etcd/ || echo "No etcd certs"
#      register: certs_check
#      when: inventory_hostname == "master-node-1"
#
#    - name: Отображение проверки сертификатов
#      debug:
#        var: certs_check.stdout
#      when: inventory_hostname == "master-node-1"

- name: Копирование сертификатов через архив
  block:
    - name: Создание tar архива с сертификатами на master-node-1
      shell: |
        tar -czf /tmp/k8s_certs.tar.gz -C /etc/kubernetes/pki .
      delegate_to: master-node-1
      become: yes
      run_once: true

    - name: Скачивание архива с сертификатами
      fetch:
        src: /tmp/k8s_certs.tar.gz
        dest: "{{ playbook_dir }}/k8s_certs.tar.gz"
        flat: yes
      become: yes
      run_once: true
      when: inventory_hostname == "master-node-1"

    - name: Очистка архива на master-node-1
      file:
        path: /tmp/k8s_certs.tar.gz
        state: absent
      delegate_to: master-node-1
      become: yes
      run_once: true

    - name: Распаковка архива на другие мастера
      unarchive:
        src: "{{ playbook_dir }}/k8s_certs.tar.gz"
        dest: /etc/kubernetes/pki
        owner: root
        group: root
      become: yes
      when: inventory_hostname != "master-node-1"

    - name: Установка правильных прав на ключевые файлы
      file:
        path: "/etc/kubernetes/pki/{{ item }}"
        mode: '0600'
        owner: root
        group: root
      loop:
        - ca.key
        - sa.key
        - front-proxy-ca.key
        - front-proxy-client.key
        - apiserver.key
        - apiserver-kubelet-client.key
        - apiserver-etcd-client.key
      become: yes
      when: inventory_hostname != "master-node-1"

    - name: Проверка скопированных сертификатов
      shell: |
        echo "=== Сертификаты в /etc/kubernetes/pki/ ==="
        ls -la /etc/kubernetes/pki/
        echo "=== Сертификаты etcd ==="
        ls -la /etc/kubernetes/pki/etcd/ || echo "No etcd certs"
      register: certs_check
      become: yes
      when: inventory_hostname != "master-node-1"

    - name: Отображение проверки сертификатов
      debug:
        var: certs_check.stdout
      when: inventory_hostname != "master-node-1"

    - name: Очистка архива на хосте Ansible
      file:
        path: "{{ playbook_dir }}/k8s_certs.tar.gz"
        state: absent
      delegate_to: localhost
      run_once: true

- name: Инициализация первого control-plane узла
  block:
    - name: Загрузка необходимых образов
      command: kubeadm config images pull --config /home/{{ k8s_user }}/kubeadm-config.yaml
      when: inventory_hostname == "master-node-1"

    - name: Полное удаление manifests директории со всем содержимым
      file:
        path: /etc/kubernetes/manifests/
        state: absent
      become: yes

    - name: Пересоздание manifest директории
      file:
        path: /etc/kubernetes/manifests/
        state: directory
        owner: root
        group: root
        mode: '0755'
      become: yes

    - name: Иницализация конфига kubelet
      command: kubeadm init phase kubeconfig kubelet --config /home/{{ k8s_user }}/kubeadm-config.yaml
      when: inventory_hostname == "master-node-1"
      become: yes

    - name: Создание systemd сервиса для kubelet
      copy:
        content: |
          [Unit]
          Description=Kubernetes Kubelet
          Documentation=https://kubernetes.io/docs/home/
          Wants=network-online.target
          After=network-online.target
          After=containerd.service
          
          [Service]
          ExecStart=/usr/bin/kubelet \
            --config=/var/lib/kubelet/config.yaml \
            --kubeconfig=/etc/kubernetes/kubelet.conf \
            --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \
            --pod-infra-container-image={{ registry_url }}/pause:3.9 \
            --fail-swap-on=false \
            --v=2
          Restart=always
          StartLimitInterval=0
          RestartSec=10
          
          [Install]
          WantedBy=multi-user.target
        dest: /lib/systemd/system/kubelet.service
        owner: root
        group: root
        mode: '0644'
      become: yes
      when: inventory_hostname == "master-node-1"

    - name: Перезагрузка демона systemd
      systemd:
        daemon_reload: yes
      become: yes
      when: inventory_hostname == "master-node-1"

    - name: Stop kubelet service
      become: yes
      systemd:
        name: kubelet
        state: stopped

    - name: Инициализация кластера с external etcd
      command: |
        kubeadm init --config /home/{{ k8s_user }}/kubeadm-config.yaml --upload-certs -v=5
      register: kubeadm_init
      when: inventory_hostname == "master-node-1"

    - name: Отображение результата инициализации
      debug:
        var: kubeadm_init.stdout
      when: inventory_hostname == "master-node-1"

    - name: Извлечение join команд (упрощенный способ)
      block:
        - name: Найти все строки с join командой
          set_fact:
            all_join_lines: "{{ kubeadm_init.stdout | regex_findall('kubeadm join[^}]+') }}"
          when: inventory_hostname == "master-node-1"

        - name: Найти control-plane команду
          set_fact:
            control_plane_command: "{{ all_join_lines | select('match', '--control-plane') | first | replace('\\n', ' ') | replace('\\', '') | trim }}"
          when: inventory_hostname == "master-node-1"

        - name: Извлечение компонентов из команды
          set_fact:
            certificate_key: "{{ kubeadm_init.stdout | regex_search('certificate key:[\\s]*([a-f0-9]{64})') | regex_replace('.*certificate key:[\\s]*([a-f0-9]{64}).*', '\\\\1') }}"
            join_token: "{{ control_plane_command | regex_search('--token\\s+([a-z0-9]{6}\\.[a-z0-9]{16})') | regex_replace('.*--token\\s+([a-z0-9]{6}\\.[a-z0-9]{16}).*', '\\\\1') }}"
            ca_cert_hash: "{{ control_plane_command | regex_search('--discovery-token-ca-cert-hash\\s+(sha256:[a-f0-9]{64})') | regex_replace('.*--discovery-token-ca-cert-hash\\s+(sha256:[a-f0-9]{64}).*', '\\\\1') }}"
          when: inventory_hostname == "master-node-1"

    - name: Проверка извлеченных данных
      debug:
        msg: |
          Certificate Key: {{ certificate_key }}
          Join Token: {{ join_token }}
          CA Cert Hash: {{ ca_cert_hash }}
      when: inventory_hostname == "master-node-1"

    - name: Создание control plane join команды
      set_fact:
        control_plane_join_command: "kubeadm join {{ cluster_vip }}:6443 --token {{ join_token }} --discovery-token-ca-cert-hash {{ ca_cert_hash }} --control-plane --certificate-key {{ certificate_key }}"
      when: inventory_hostname == "master-node-1"

    - name: Отображение итоговой join команды
      debug:
        msg: |
          === CONTROL PLANE JOIN COMMAND ===
          {{ control_plane_join_command }}
      when: inventory_hostname == "master-node-1"

    - name: Настройка kubectl для пользователя
      block:
        - name: Создание .kube директории
          file:
            path: "{{ k8s_home }}/.kube"
            state: directory
            owner: "{{ k8s_user }}"
            group: "{{ k8s_group }}"
            mode: 0750

        - name: Копирование конфигурации
          copy:
            src: /etc/kubernetes/admin.conf
            dest: "{{ k8s_home }}/.kube/config"
            remote_src: yes
            owner: "{{ k8s_user }}"
            group: "{{ k8s_group }}"
            mode: 0600
      when: inventory_hostname == "master-node-1"

- name: Ожидание готовности первого control-plane узла
  block:
    - name: Проверка здоровья Kubernetes API
      shell: |
        curl -k -s -o /dev/null -w "%{http_code}" https://127.0.0.1:6443/healthz
      register: api_health_check
      failed_when: api_health_check.stdout != "200"
      retries: 30
      delay: 10
      when: inventory_hostname == "master-node-1"

    - name: Проверка компонентов control plane
      command: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l tier=control-plane
      register: control_plane_pods
      when: inventory_hostname == "master-node-1"

    - name: Отображение статуса control plane
      debug:
        var: control_plane_pods.stdout
      when: inventory_hostname == "master-node-1"

- name: Установка CNI плагина (Flannel)
  block:
    - name: Применение Flannel
      command: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      when: inventory_hostname == "master-node-1"

    - name: Ожидание запуска Flannel
      command: kubectl wait --for=condition=ready pod -l app=flannel -n kube-flannel --timeout=300s
      when: inventory_hostname == "master-node-1"

- name: Присоединение остальных control-plane узлов
  block:
    - name: Присоединение control-plane узла
      command: "{{ control_plane_join_command }}"
      when:
        - inventory_hostname != "master-node-1"
        - inventory_hostname in groups['masters']

    - name: Настройка kubectl на других control-plane узлах
      block:
        - name: Создание .kube директории
          file:
            path: "{{ k8s_home }}/.kube"
            state: directory
            owner: "{{ k8s_user }}"
            group: "{{ k8s_group }}"
            mode: 0750

        - name: Копирование конфигурации с первого узла
          slurp:
            src: "{{ k8s_home }}/.kube/config"
          register: kubeconfig
          delegate_to: master-node-1

        - name: Запись конфигурации
          copy:
            content: "{{ kubeconfig.content | b64decode }}"
            dest: "{{ k8s_home }}/.kube/config"
            owner: "{{ k8s_user }}"
            group: "{{ k8s_group }}"
            mode: 0600
      when:
        - inventory_hostname != "master-node-1"
        - inventory_hostname in groups['masters']

- name: Проверка состояния кластера
  block:
    - name: Проверка узлов
      command: kubectl get nodes -o wide
      register: nodes_info
      when: inventory_hostname == "master-node-1"

    - name: Отображение информации об узлах
      debug:
        var: nodes_info.stdout
      when: inventory_hostname == "master-node-1"

    - name: Проверка подключения к etcd
      shell: |
        ETCDCTL_API=3 etcdctl \
          --endpoints=https://{{ hostvars['master-node-1'].internal_ip }}:2379,https://{{ hostvars['master-node-2'].internal_ip }}:2379,https://{{ hostvars['master-node-3'].internal_ip }}:2379 \
          --cacert=/etc/kubernetes/pki/etcd-ca.crt \
          --cert=/etc/kubernetes/pki/apiserver-etcd-client.crt \
          --key=/etc/kubernetes/pki/apiserver-etcd-client.key \
          endpoint health
      register: etcd_health
      ignore_errors: yes
      when: inventory_hostname == "master-node-1"

    - name: Отображение статуса etcd
      debug:
        var: etcd_health.stdout
      when: inventory_hostname == "master-node-1"