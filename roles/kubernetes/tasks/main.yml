---
- name: Установка и настройка containerd
  block:
    - name: Копирование rpm пакета containerd на сервер
      copy:
        src: "files/packages/containerd-2.1.4-alt1.x86_64.rpm"
        dest: /tmp/containerd.rpm
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Установка containerd через apt-get
      shell: apt-get install -y /tmp/containerd.rpm
      become: yes

    - name: Очистка временного файла
      file:
        path: /tmp/containerd.rpm
        state: absent
      become: yes

    - name: Создание базового конфига containerd
      copy:
        content: |
          version = 2
          root = "/var/lib/containerd"
          state = "/run/containerd"
          [grpc]
            address = "/run/containerd/containerd.sock"
          [plugins]
            [plugins."io.containerd.grpc.v1.cri"]
              sandbox_image = "{{ registry_url }}/pause:3.9"
              [plugins."io.containerd.grpc.v1.cri".registry]
                [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."89.208.208.139:5000"]
                    endpoint = ["http://89.208.208.139:5000"]
                [plugins."io.containerd.grpc.v1.cri".registry.configs]
                  [plugins."io.containerd.grpc.v1.cri".registry.configs."89.208.208.139:5000".tls]
                    insecure_skip_verify = true
              [plugins."io.containerd.grpc.v1.cri".containerd]
                snapshotter = "overlayfs"
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                  runtime_type = "io.containerd.runc.v2"
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                    SystemdCgroup = true
        dest: /etc/containerd/config.toml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Включение и запуск containerd
      systemd:
        name: containerd
        state: started
        enabled: yes
        daemon_reload: yes
      become: yes

    - name: Ожидание готовности containerd
      wait_for:
        path: /var/run/containerd/containerd.sock
        state: present
        timeout: 30
      become: yes

    - name: Проверка работы containerd
      command: crictl version
      register: containerd_check
      changed_when: false
      failed_when: containerd_check.rc != 0

- name: Установка и настройка crictl
  block:
    - name: Копирование и распаковка crictl
      unarchive:
        src: "files/binares/crictl-v1.28.0-linux-amd64.tar.gz"
        dest: /usr/bin
        owner: root
        group: root
        mode: '0755'
        remote_src: no
      become: yes

    - name: Настройка конфига crictl
      copy:
        content: |
          runtime-endpoint: unix:///var/run/containerd/containerd.sock
          image-endpoint: unix:///var/run/containerd/containerd.sock
          timeout: 10
          debug: false
        dest: /etc/crictl.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Проверка работы crictl
      command: crictl --version
      register: crictl_result
      changed_when: false

- name: Обновление apt cache
  shell: apt-get update
  become: yes

- name: Установка Kubernetes компонентов
  shell: |
    apt-get install -y \
      kubernetes1.28-client \
      kubernetes1.28-kubeadm \
      kubernetes1.28-kubelet \
      kubernetes1.28-master \
      kubernetes1.28-node \
      kubernetes1.28-common \
      kubernetes1.28-crio \
      python3-module-kubernetes-client \
      kube-vip
  become: yes

- name: Полная очистка предыдущей конфигурации Kubernetes
  block:
    - name: Остановка kubelet
      systemd:
        name: kubelet
        state: stopped
      become: yes

    - name: Удаление всех контейнеров
      shell: |
        crictl ps -aq | xargs -I {} crictl stop {} 2>/dev/null || true
        crictl ps -aq | xargs -I {} crictl rm {} 2>/dev/null || true
        sleep 5
      become: yes

    - name: Полный сброс kubeadm
      shell: kubeadm reset -f
      become: yes
      ignore_errors: yes

    - name: Очистка всех конфигурационных файлов
      shell: |
        rm -rf /etc/kubernetes
        rm -rf /var/lib/etcd
        rm -rf /var/lib/kubelet
        rm -rf /var/lib/cni
        rm -rf /etc/cni
        rm -rf $HOME/.kube
      become: yes

    #- name: Очистка сетевых интерфейсов
    #  shell: |
    #    ip link delete cni0 2>/dev/null || true
    #    ip link delete flannel.1 2>/dev/null || true
    #    ip link delete docker0 2>/dev/null || true
    #    iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
    #  args:
    #    warn: false
    #  become: yes

    - name: Перезапуск containerd
      systemd:
        name: containerd
        state: started
      become: yes

    - name: Создание чистых директорий
      file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0700'
      loop:
        - /etc/kubernetes
        - /var/lib/etcd
        - /var/lib/kubelet
        - /etc/kubernetes/manifests
        - /etc/kubernetes/pki
      become: yes
  when: inventory_hostname == "master-node-1"

- name: Создание конфигурации kubeadm для первого мастера
  template:
    src: kubeadm-config.yaml.j2
    dest: "/home/{{ k8s_user }}/kubeadm-config.yaml"
    owner: "{{ k8s_user }}"
    group: "{{ k8s_group }}"
    mode: 0644
  when: inventory_hostname == "master-node-1"

- name: Генерация сертификатов
  command: |
    kubeadm init phase certs all --config /home/{{ k8s_user }}/kubeadm-config.yaml
  when: inventory_hostname == "master-node-1"

- name: Создание kubeconfig файлов
  command: |
    kubeadm init phase kubeconfig all --config /home/{{ k8s_user }}/kubeadm-config.yaml
  when: inventory_hostname == "master-node-1"

- name: Ручное создание манифеста etcd
  copy:
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://{{ master_ips[inventory_hostname] }}:2379
        creationTimestamp: null
        labels:
          component: etcd
          tier: control-plane
        name: etcd
        namespace: kube-system
      spec:
        containers:
        - command:
          - etcd
          - --advertise-client-urls=https://{{ master_ips[inventory_hostname] }}:2379
          - --cert-file=/etc/kubernetes/pki/etcd/server.crt
          - --client-cert-auth=true
          - --data-dir=/var/lib/etcd
          - --initial-advertise-peer-urls=https://{{ master_ips[inventory_hostname] }}:2380
          - --initial-cluster=master-node-1=https://{{ master_ips[inventory_hostname] }}:2380
          - --key-file=/etc/kubernetes/pki/etcd/server.key
          - --listen-client-urls=https://0.0.0.0:2379
          - --listen-metrics-urls=http://0.0.0.0:2381
          - --listen-peer-urls=https://0.0.0.0:2380
          - --name=master-node-1
          - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
          - --peer-client-cert-auth=true
          - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
          - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
          - --snapshot-count=10000
          - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
          image: {{ registry_url }}/etcd:3.5.9-0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              host: 127.0.0.1
              path: /health
              port: 2381
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          name: etcd
          resources: {}
          startupProbe:
            failureThreshold: 24
            httpGet:
              host: 127.0.0.1
              path: /health
              port: 2381
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /var/lib/etcd
            name: etcd-data
          - mountPath: /etc/kubernetes/pki/etcd
            name: etcd-certs
        hostNetwork: true
        priorityClassName: system-node-critical
        volumes:
        - hostPath:
            path: /var/lib/etcd
            type: DirectoryOrCreate
          name: etcd-data
        - hostPath:
            path: /etc/kubernetes/pki/etcd
            type: DirectoryOrCreate
          name: etcd-certs
    dest: /etc/kubernetes/manifests/etcd.yaml
    owner: root
    group: root
    mode: '0644'
  become: yes
  when: inventory_hostname == "master-node-1"

- name: Создание systemd сервиса для kubelet
  copy:
    content: |
      [Unit]
      Description=Kubernetes Kubelet
      Documentation=https://kubernetes.io/docs/home/
      Wants=network-online.target
      After=network-online.target
      After=containerd.service

      [Service]
      ExecStart=/usr/bin/kubelet \
        --config=/var/lib/kubelet/config.yaml \
        --pod-infra-container-image={{ registry_url }}/pause:3.9 \
        --fail-swap-on=false \
        --v=2
      Restart=always
      StartLimitInterval=0
      RestartSec=10

      [Install]
      WantedBy=multi-user.target
    dest: /lib/systemd/system/kubelet.service
    owner: root
    group: root
    mode: '0644'
  become: yes

- name: Создание базовой конфигурации kubelet
  copy:
    content: |
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      cgroupDriver: systemd
      clusterDomain: cluster.local
      clusterDNS:
      - 10.96.0.10
      staticPodPath: /etc/kubernetes/manifests
      failSwapOn: false
    dest: /var/lib/kubelet/config.yaml
    owner: root
    group: root
    mode: '0644'
  become: yes

- name: Перезагрузка демона systemd
  systemd:
    daemon_reload: yes
  become: yes

- name: Запуск kubelet
  systemd:
    name: kubelet
    state: started
    enabled: yes
  become: yes

- name: Ожидание запуска etcd
  shell: |
    timeout 300 bash -c '
      until crictl ps --name etcd --state running 2>/dev/null | grep -q etcd; do 
        echo "Waiting for etcd..."; 
        sleep 10; 
      done
      echo "etcd container is running"
    
      until crictl logs $(crictl ps --name etcd -q) 2>/dev/null | grep -q "ready to serve client requests"; do
        echo "Waiting for etcd to be ready...";
        sleep 10;
      done
      echo "etcd is ready to serve requests"
    '
  register: etcd_wait
  when: inventory_hostname == "master-node-1"

- name: Создание манифеста kube-apiserver
  copy:
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        creationTimestamp: null
        labels:
          component: kube-apiserver
          tier: control-plane
        name: kube-apiserver
        namespace: kube-system
      spec:
        containers:
        - command:
          - kube-apiserver
          - --advertise-address={{ master_ips[inventory_hostname] }}
          - --allow-privileged=true
          - --authorization-mode=Node,RBAC
          - --client-ca-file=/etc/kubernetes/pki/ca.crt
          - --enable-admission-plugins=NodeRestriction
          - --enable-bootstrap-token-auth=true
          - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
          - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
          - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
          - --etcd-servers=https://127.0.0.1:2379
          - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
          - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
          - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
          - --requestheader-allowed-names=front-proxy-client
          - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
          - --requestheader-extra-headers-prefix=X-Remote-Extra-
          - --requestheader-group-headers=X-Remote-Group
          - --requestheader-username-headers=X-Remote-User
          - --secure-port=6443
          - --service-account-issuer=https://kubernetes.default.svc.cluster.local
          - --service-account-key-file=/etc/kubernetes/pki/sa.pub
          - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
          - --service-cluster-ip-range={{ service_cidr }}
          - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
          - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
          image: {{ registry_url }}/kube-apiserver:v1.28.15
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              host: 127.0.0.1
              path: /livez
              port: 6443
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          name: kube-apiserver
          resources:
            requests:
              cpu: 250m
          startupProbe:
            failureThreshold: 24
            httpGet:
              host: 127.0.0.1
              path: /livez
              port: 6443
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ca-certs
            readOnly: true
          - mountPath: /etc/ca-certificates
            name: etc-pki
            readOnly: true
          - mountPath: /etc/kubernetes/pki
            name: k8s-certs
            readOnly: true
          - mountPath: /usr/share/ca-certificates
            name: usr-share-ca-certificates
            readOnly: true
        hostNetwork: true
        priorityClassName: system-node-critical
        volumes:
        - hostPath:
            path: /etc/ssl/certs
            type: DirectoryOrCreate
          name: ca-certs
        - hostPath:
            path: /etc/pki
            type: DirectoryOrCreate
          name: etc-pki
        - hostPath:
            path: /etc/kubernetes/pki
            type: DirectoryOrCreate
          name: k8s-certs
        - hostPath:
            path: /usr/share/ca-certificates
            type: DirectoryOrCreate
          name: usr-share-ca-certificates
    dest: /etc/kubernetes/manifests/kube-apiserver.yaml
    owner: root
    group: root
    mode: '0644'
  become: yes
  when: inventory_hostname == "master-node-1"

- name: Ожидание запуска kube-apiserver
  shell: |
    timeout 300 bash -c '
      until crictl ps --name kube-apiserver --state running 2>/dev/null | grep -q kube-apiserver; do 
        echo "Waiting for kube-apiserver..."; 
        sleep 10; 
      done
      echo "kube-apiserver is running"
    '
  register: api_wait
  when: inventory_hostname == "master-node-1"

- name: Ожидание готовности API
  shell: |
    timeout 300 bash -c '
      until curl -k https://127.0.0.1:6443/healthz 2>/dev/null; do
        echo "Waiting for API server to be healthy...";
        sleep 10;
      done
      echo "API server is healthy"
    '
  when: inventory_hostname == "master-node-1"

- name: Копирование конфигурации для пользователя
  copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ k8s_home }}/.kube/config"
    remote_src: yes
    owner: "{{ k8s_user }}"
    group: "{{ k8s_group }}"
    mode: 0600
  when: inventory_hostname == "master-node-1"

- name: Создание остальных control-plane компонентов
  command: |
    kubeadm init phase control-plane controller-manager --config /home/{{ k8s_user }}/kubeadm-config.yaml
  when: inventory_hostname == "master-node-1"

- name: Создание scheduler
  command: |
    kubeadm init phase control-plane scheduler --config /home/{{ k8s_user }}/kubeadm-config.yaml
  when: inventory_hostname == "master-node-1"

- name: Завершение инициализации
  command: |
    kubeadm init phase upload-config all --config /home/{{ k8s_user }}/kubeadm-config.yaml
  when: inventory_hostname == "master-node-1"

- name: Отметка узла как готового
  command: |
    kubectl taint nodes --all node-role.kubernetes.io/control-plane-
  when: inventory_hostname == "master-node-1"

- name: Установка CNI плагина
  command: |
    kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
  when: inventory_hostname == "master-node-1"

- name: Получение команды join
  command: kubeadm token create --print-join-command
  register: join_command
  changed_when: false
  when: inventory_hostname == "master-node-1"

- name: Получение certificate key
  command: kubeadm init phase upload-certs --upload-certs
  register: cert_key
  changed_when: false
  when: inventory_hostname == "master-node-1"

- name: Сохранение команд join в переменные
  set_fact:
    control_plane_join_command: "kubeadm join {{ cluster_vip }}:6443 --token {{ join_command.stdout.split()[3] }} --discovery-token-ca-cert-hash {{ join_command.stdout.split()[5] }} --control-plane --certificate-key {{ cert_key.stdout.split()[3] }}"
    worker_join_command: "kubeadm join {{ cluster_vip }}:6443 --token {{ join_command.stdout.split()[3] }} --discovery-token-ca-cert-hash {{ join_command.stdout.split()[5] }}"
  when: inventory_hostname == "master-node-1"

- name: Присоединение других мастер-узлов
  command: "{{ hostvars['master-node-1']['control_plane_join_command'] }}"
  when:
    - inventory_hostname != "master-node-1"
    - inventory_hostname in groups['masters']

- name: Настройка kubectl на других мастерах
  block:
    - name: Копирование конфигурации с первого мастера
      slurp:
        src: "{{ k8s_home }}/.kube/config"
      register: kubeconfig
      delegate_to: master-node-1

    - name: Запись конфигурации на текущий узел
      copy:
        content: "{{ kubeconfig.content | b64decode }}"
        dest: "{{ k8s_home }}/.kube/config"
        owner: "{{ k8s_user }}"
        group: "{{ k8s_group }}"
        mode: 0600
  when:
    - inventory_hostname != "master-node-1"
    - inventory_hostname in groups['masters']