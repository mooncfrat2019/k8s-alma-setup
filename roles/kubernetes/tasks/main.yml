---
- name: Установка и настройка containerd
  block:
    - name: Копирование rpm пакета containerd на сервер
      copy:
        src: "files/packages/containerd-2.1.4-alt1.x86_64.rpm"
        dest: /tmp/containerd.rpm
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Установка containerd через apt-get
      shell: apt-get install -y /tmp/containerd.rpm
      become: yes

    - name: Очистка временного файла
      file:
        path: /tmp/containerd.rpm
        state: absent
      become: yes

    - name: Создание базового конфига containerd
      copy:
        content: |
          version = 2
          root = "/var/lib/containerd"
          state = "/run/containerd"
          [grpc]
            address = "/run/containerd/containerd.sock"
          [plugins]
            [plugins."io.containerd.grpc.v1.cri"]
              sandbox_image = "{{ registry_url }}/pause:3.9"
              [plugins."io.containerd.grpc.v1.cri".registry]
                [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
                  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."89.208.208.139:5000"]
                    endpoint = ["http://89.208.208.139:5000"]
                [plugins."io.containerd.grpc.v1.cri".registry.configs]
                  [plugins."io.containerd.grpc.v1.cri".registry.configs."89.208.208.139:5000".tls]
                    insecure_skip_verify = true
              [plugins."io.containerd.grpc.v1.cri".containerd]
                snapshotter = "overlayfs"
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                  runtime_type = "io.containerd.runc.v2"
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                    SystemdCgroup = true
        dest: /etc/containerd/config.toml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Включение и запуск containerd
      systemd:
        name: containerd
        state: started
        enabled: yes
        daemon_reload: yes
      become: yes

    - name: Ожидание готовности containerd
      wait_for:
        path: /var/run/containerd/containerd.sock
        state: present
        timeout: 30
      become: yes

    - name: Проверка работы containerd
      command: crictl version
      register: containerd_check
      changed_when: false
      failed_when: containerd_check.rc != 0

- name: Установка и настройка crictl
  block:
    - name: Копирование и распаковка crictl
      unarchive:
        src: "files/binares/crictl-v1.28.0-linux-amd64.tar.gz"
        dest: /usr/bin
        owner: root
        group: root
        mode: '0755'
        remote_src: no
      become: yes

    - name: Настройка конфига crictl
      copy:
        content: |
          runtime-endpoint: unix:///var/run/containerd/containerd.sock
          image-endpoint: unix:///var/run/containerd/containerd.sock
          timeout: 10
          debug: false
        dest: /etc/crictl.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Проверка работы crictl
      command: crictl --version
      register: crictl_result
      changed_when: false

- name: Обновление apt cache
  shell: apt-get update
  become: yes

- name: Установка Kubernetes компонентов
  shell: |
    apt-get install -y \
      kubernetes1.28-client \
      kubernetes1.28-kubeadm \
      kubernetes1.28-kubelet \
      kubernetes1.28-master \
      kubernetes1.28-node \
      kubernetes1.28-common \
      kubernetes1.28-crio \
      python3-module-kubernetes-client \
      kube-vip
  become: yes

- name: Подготовка директорий для Kubernetes
  block:
    - name: Остановка kubelet если запущен
      systemd:
        name: kubelet
        state: stopped
      become: yes
      ignore_errors: yes

    - name: Создание необходимых директорий
      file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - /etc/kubernetes
        - /var/lib/etcd
        - /var/lib/kubelet
        - /etc/kubernetes/manifests
        - /etc/kubernetes/pki
      become: yes
  when: inventory_hostname == "master-node-1"

- name: Создание конфигурации kubeadm для первого мастера
  template:
    src: kubeadm-config.yaml.j2
    dest: "/home/{{ k8s_user }}/kubeadm-config.yaml"
    owner: "{{ k8s_user }}"
    group: "{{ k8s_group }}"
    mode: 0644
  when: inventory_hostname == "master-node-1"

- name: Генерация сертификатов Kubernetes
  command: |
    kubeadm init phase certs all --config /home/{{ k8s_user }}/kubeadm-config.yaml
  when: inventory_hostname == "master-node-1"

- name: Создание kubeconfig файлов
  command: |
    kubeadm init phase kubeconfig all --config /home/{{ k8s_user }}/kubeadm-config.yaml
  when: inventory_hostname == "master-node-1"

- name: Создание ВСЕХ манифестов static pods перед запуском kubelet
  block:
    - name: Создание манифеста etcd
      copy:
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            annotations:
              kubeadm.kubernetes.io/etcd.advertise-client-urls: https://{{ master_ips[inventory_hostname] }}:2379
            creationTimestamp: null
            labels:
              component: etcd
              tier: control-plane
            name: etcd
            namespace: kube-system
          spec:
            containers:
            - command:
              - etcd
              - --advertise-client-urls=https://{{ master_ips[inventory_hostname] }}:2379
              - --cert-file=/etc/kubernetes/pki/etcd/server.crt
              - --client-cert-auth=true
              - --data-dir=/var/lib/etcd
              - --initial-advertise-peer-urls=https://{{ master_ips[inventory_hostname] }}:2380
              - --initial-cluster=master-node-1=https://{{ master_ips[inventory_hostname] }}:2380
              - --key-file=/etc/kubernetes/pki/etcd/server.key
              - --listen-client-urls=https://0.0.0.0:2379
              - --listen-metrics-urls=http://0.0.0.0:2381
              - --listen-peer-urls=https://0.0.0.0:2380
              - --name=master-node-1
              - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
              - --peer-client-cert-auth=true
              - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
              - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
              - --snapshot-count=10000
              - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
              image: {{ registry_url }}/etcd:3.5.15-0
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 8
                httpGet:
                  host: 127.0.0.1
                  path: /health
                  port: 2381
                  scheme: HTTP
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              name: etcd
              resources: {}
              startupProbe:
                failureThreshold: 24
                httpGet:
                  host: 127.0.0.1
                  path: /health
                  port: 2381
                  scheme: HTTP
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /var/lib/etcd
                name: etcd-data
              - mountPath: /etc/kubernetes/pki/etcd
                name: etcd-certs
            hostNetwork: true
            priorityClassName: system-node-critical
            volumes:
            - hostPath:
                path: /var/lib/etcd
                type: DirectoryOrCreate
              name: etcd-data
            - hostPath:
                path: /etc/kubernetes/pki/etcd
                type: DirectoryOrCreate
              name: etcd-certs
        dest: /etc/kubernetes/manifests/etcd.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Создание манифеста kube-apiserver
      copy:
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            labels:
              component: kube-apiserver
              tier: control-plane
            name: kube-apiserver
            namespace: kube-system
          spec:
            containers:
            - command:
              - kube-apiserver
              - --advertise-address={{ master_ips[inventory_hostname] }}
              - --allow-privileged=true
              - --authorization-mode=Node,RBAC
              - --client-ca-file=/etc/kubernetes/pki/ca.crt
              - --enable-admission-plugins=NodeRestriction
              - --enable-bootstrap-token-auth=true
              - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
              - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
              - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
              - --etcd-servers=https://127.0.0.1:2379
              - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
              - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
              - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
              - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
              - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
              - --requestheader-allowed-names=front-proxy-client
              - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
              - --requestheader-extra-headers-prefix=X-Remote-Extra-
              - --requestheader-group-headers=X-Remote-Group
              - --requestheader-username-headers=X-Remote-User
              - --secure-port=6443
              - --service-account-issuer=https://kubernetes.default.svc.cluster.local
              - --service-account-key-file=/etc/kubernetes/pki/sa.pub
              - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
              - --service-cluster-ip-range={{ service_cidr }}
              - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
              - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
              image: {{ registry_url }}/kube-apiserver:v1.28.15
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 8
                httpGet:
                  host: 127.0.0.1
                  path: /livez
                  port: 6443
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              name: kube-apiserver
              resources:
                requests:
                  cpu: 250m
              startupProbe:
                failureThreshold: 24
                httpGet:
                  host: 127.0.0.1
                  path: /livez
                  port: 6443
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /etc/ssl/certs
                name: ca-certs
                readOnly: true
              - mountPath: /etc/ca-certificates
                name: etc-pki
                readOnly: true
              - mountPath: /etc/kubernetes/pki
                name: k8s-certs
                readOnly: true
              - mountPath: /usr/share/ca-certificates
                name: usr-share-ca-certificates
                readOnly: true
            hostNetwork: true
            priorityClassName: system-node-critical
            volumes:
            - hostPath:
                path: /etc/ssl/certs
                type: DirectoryOrCreate
              name: ca-certs
            - hostPath:
                path: /etc/pki
                type: DirectoryOrCreate
              name: etc-pki
            - hostPath:
                path: /etc/kubernetes/pki
                type: DirectoryOrCreate
              name: k8s-certs
            - hostPath:
                path: /usr/share/ca-certificates
                type: DirectoryOrCreate
              name: usr-share-ca-certificates
        dest: /etc/kubernetes/manifests/kube-apiserver.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Создание манифеста kube-controller-manager
      copy:
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            labels:
              component: kube-controller-manager
              tier: control-plane
            name: kube-controller-manager
            namespace: kube-system
          spec:
            containers:
            - command:
              - kube-controller-manager
              - --allocate-node-cidrs=true
              - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
              - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
              - --bind-address=127.0.0.1
              - --client-ca-file=/etc/kubernetes/pki/ca.crt
              - --cluster-cidr={{ pod_network_cidr }}
              - --cluster-name=kubernetes
              - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
              - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
              - --controllers=*,bootstrapsigner,tokencleaner
              - --kubeconfig=/etc/kubernetes/controller-manager.conf
              - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
              - --root-ca-file=/etc/kubernetes/pki/ca.crt
              - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
              - --use-service-account-credentials=true
              image: {{ registry_url }}/kube-controller-manager:v1.28.15
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 8
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10257
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              name: kube-controller-manager
              resources:
                requests:
                  cpu: 200m
              startupProbe:
                failureThreshold: 24
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10257
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /etc/ssl/certs
                name: ca-certs
                readOnly: true
              - mountPath: /etc/ca-certificates
                name: etc-pki
                readOnly: true
              - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
                name: flexvolume-dir
              - mountPath: /etc/kubernetes/pki
                name: k8s-certs
                readOnly: true
              - mountPath: /etc/kubernetes/controller-manager.conf
                name: kubeconfig
                readOnly: true
              - mountPath: /usr/share/ca-certificates
                name: usr-share-ca-certificates
                readOnly: true
            hostNetwork: true
            priorityClassName: system-node-critical
            volumes:
            - hostPath:
                path: /etc/ssl/certs
                type: DirectoryOrCreate
              name: ca-certs
            - hostPath:
                path: /etc/pki
                type: DirectoryOrCreate
              name: etc-pki
            - hostPath:
                path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
                type: DirectoryOrCreate
              name: flexvolume-dir
            - hostPath:
                path: /etc/kubernetes/pki
                type: DirectoryOrCreate
              name: k8s-certs
            - hostPath:
                path: /etc/kubernetes/controller-manager.conf
                type: FileOrCreate
              name: kubeconfig
            - hostPath:
                path: /usr/share/ca-certificates
                type: DirectoryOrCreate
              name: usr-share-ca-certificates
        dest: /etc/kubernetes/manifests/kube-controller-manager.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes

    - name: Создание манифеста kube-scheduler
      copy:
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            labels:
              component: kube-scheduler
              tier: control-plane
            name: kube-scheduler
            namespace: kube-system
          spec:
            containers:
            - command:
              - kube-scheduler
              - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
              - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
              - --bind-address=127.0.0.1
              - --kubeconfig=/etc/kubernetes/scheduler.conf
              image: {{ registry_url }}/kube-scheduler:v1.28.15
              imagePullPolicy: IfNotPresent
              livenessProbe:
                failureThreshold: 8
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10259
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              name: kube-scheduler
              resources:
                requests:
                  cpu: 100m
              startupProbe:
                failureThreshold: 24
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10259
                  scheme: HTTPS
                initialDelaySeconds: 10
                periodSeconds: 10
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /etc/kubernetes/scheduler.conf
                name: kubeconfig
                readOnly: true
            hostNetwork: true
            priorityClassName: system-node-critical
            volumes:
            - hostPath:
                path: /etc/kubernetes/scheduler.conf
                type: FileOrCreate
              name: kubeconfig
        dest: /etc/kubernetes/manifests/kube-scheduler.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes
  when: inventory_hostname == "master-node-1"

- name: Создание systemd сервиса для kubelet
  copy:
    content: |
      [Unit]
      Description=Kubernetes Kubelet
      Documentation=https://kubernetes.io/docs/home/
      Wants=network-online.target
      After=network-online.target
      After=containerd.service

      [Service]
      ExecStart=/usr/bin/kubelet \
        --config=/var/lib/kubelet/config.yaml \
        --kubeconfig=/etc/kubernetes/kubelet.conf \
        --pod-infra-container-image={{ registry_url }}/pause:3.9 \
        --fail-swap-on=false \
        --v=2
      Restart=always
      StartLimitInterval=0
      RestartSec=10

      [Install]
      WantedBy=multi-user.target
    dest: /lib/systemd/system/kubelet.service
    owner: root
    group: root
    mode: '0644'
  become: yes

- name: Создание базовой конфигурации kubelet
  copy:
    content: |
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      cgroupDriver: systemd
      clusterDomain: cluster.local
      clusterDNS:
      - 10.96.0.10
      staticPodPath: /etc/kubernetes/manifests
      failSwapOn: false
    dest: /var/lib/kubelet/config.yaml
    owner: root
    group: root
    mode: '0644'
  become: yes

- name: Перезагрузка демона systemd
  systemd:
    daemon_reload: yes
  become: yes

- name: Запуск kubelet
  systemd:
    name: kubelet
    state: started
    enabled: yes
  become: yes

- name: Проверка здоровья Kubernetes API
  shell: |
    curl -k -s -o /dev/null -w "%{http_code}" https://127.0.0.1:6443/healthz
  register: api_health_check
  failed_when: api_health_check.stdout != "200"
  retries: 30
  delay: 10
  when: inventory_hostname == "master-node-1"

- name: Отображение статуса проверки API
  debug:
    msg: "Kubernetes API health check: {{ api_health_check.stdout }}"
  when: inventory_hostname == "master-node-1"

- name: Копирование конфигурации для пользователя
  copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ k8s_home }}/.kube/config"
    remote_src: yes
    owner: "{{ k8s_user }}"
    group: "{{ k8s_group }}"
    mode: 0600
  when: inventory_hostname == "master-node-1"

- name: Установка kube-proxy
  block:
    - name: Создание манифеста kube-proxy
      copy:
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-proxy
            namespace: kube-system
            labels:
              k8s-app: kube-proxy
          spec:
            hostNetwork: true
            priorityClassName: system-node-critical
            containers:
            - name: kube-proxy
              image: {{ registry_url }}/kube-proxy:v1.28.15
              imagePullPolicy: IfNotPresent
              command:
              - /usr/local/bin/kube-proxy
              - --config=/var/lib/kube-proxy/config.conf
              - --hostname-override=$(NODE_NAME)
              env:
              - name: NODE_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: spec.nodeName
              securityContext:
                privileged: true
              volumeMounts:
              - mountPath: /var/lib/kube-proxy
                name: kube-proxy
              - mountPath: /run/xtables.lock
                name: xtables-lock
                readOnly: false
              - mountPath: /lib/modules
                name: lib-modules
                readOnly: true
            volumes:
            - name: kube-proxy
              hostPath:
                path: /var/lib/kube-proxy
                type: DirectoryOrCreate
            - name: xtables-lock
              hostPath:
                path: /run/xtables.lock
                type: FileOrCreate
            - name: lib-modules
              hostPath:
                path: /lib/modules
            - name: kube-proxy-config
              hostPath:
                path: /var/lib/kube-proxy/config.conf
            serviceAccountName: kube-proxy
        dest: /etc/kubernetes/manifests/kube-proxy.yaml
        owner: root
        group: root
        mode: '0644'
      become: yes
      when: inventory_hostname == "master-node-1"

    - name: Создание конфигурации kube-proxy
      copy:
        content: |
          apiVersion: kubeproxy.config.k8s.io/v1alpha1
          kind: KubeProxyConfiguration
          clientConnection:
            kubeconfig: "/var/lib/kube-proxy/kubeconfig.conf"
          clusterCIDR: "{{ pod_network_cidr }}"
          mode: "iptables"
          iptables:
            masqueradeAll: false
            masqueradeBit: 14
            minSyncPeriod: 0s
            syncPeriod: 30s
          conntrack:
            maxPerCore: 32768
            min: 131072
            tcpCloseWaitTimeout: 1h0m0s
            tcpEstablishedTimeout: 24h0m0s
        dest: /var/lib/kube-proxy/config.conf
        owner: root
        group: root
        mode: '0644'
      become: yes
      when: inventory_hostname == "master-node-1"

    - name: Создание kubeconfig для kube-proxy
      copy:
        content: |
          apiVersion: v1
          kind: Config
          clusters:
          - cluster:
              certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              server: https://127.0.0.1:6443
            name: default
          contexts:
          - context:
              cluster: default
              namespace: default
              user: default
            name: default
          current-context: default
          users:
          - name: default
            user:
              tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
        dest: /var/lib/kube-proxy/kubeconfig.conf
        owner: root
        group: root
        mode: '0644'
      become: yes
      when: inventory_hostname == "master-node-1"

- name: Установка Flannel CNI плагина
  block:
    - name: Создание манифеста Flannel
      copy:
        content: |
          ---
          apiVersion: v1
          kind: Namespace
          metadata:
            name: kube-flannel
            labels:
              pod-security.kubernetes.io/enforce: privileged
          ---
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: flannel
            namespace: kube-flannel
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: flannel
          rules:
          - apiGroups:
            - ""
            resources:
            - pods
            verbs:
            - get
          - apiGroups:
            - ""
            resources:
            - nodes
            verbs:
            - list
            - watch
          - apiGroups:
            - ""
            resources:
            - nodes/status
            verbs:
            - patch
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: flannel
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: flannel
          subjects:
          - kind: ServiceAccount
            name: flannel
            namespace: kube-flannel
          ---
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: kube-flannel-cfg
            namespace: kube-flannel
            labels:
              tier: node
              app: flannel
          data:
            cni-conf.json: |
              {
                "name": "cbr0",
                "cniVersion": "0.3.1",
                "plugins": [
                  {
                    "type": "flannel",
                    "delegate": {
                      "hairpinMode": true,
                      "isDefaultGateway": true
                    }
                  },
                  {
                    "type": "portmap",
                    "capabilities": {
                      "portMappings": true
                    }
                  }
                ]
              }
            net-conf.json: |
              {
                "Network": "{{ pod_network_cidr }}",
                "Backend": {
                  "Type": "vxlan"
                }
              }
          ---
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: kube-flannel-ds
            namespace: kube-flannel
            labels:
              tier: node
              app: flannel
          spec:
            selector:
              matchLabels:
                app: flannel
            template:
              metadata:
                labels:
                  tier: node
                  app: flannel
              spec:
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                      - matchExpressions:
                        - key: kubernetes.io/os
                          operator: In
                          values:
                          - linux
                hostNetwork: true
                priorityClassName: system-node-critical
                serviceAccountName: flannel
                tolerations:
                - operator: Exists
                  effect: NoSchedule
                containers:
                - name: kube-flannel
                  image: {{ registry_url }}/flannel:v0.27.4
                  imagePullPolicy: IfNotPresent
                  command:
                  - /opt/bin/flanneld
                  args:
                  - --ip-masq
                  - --kube-subnet-mgr
                  resources:
                    requests:
                      cpu: "100m"
                      memory: "50Mi"
                    limits:
                      memory: "50Mi"
                  securityContext:
                    privileged: false
                    capabilities:
                      add: ["NET_ADMIN", "NET_RAW"]
                  env:
                  - name: POD_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.name
                  - name: POD_NAMESPACE
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.namespace
                  volumeMounts:
                  - name: run
                    mountPath: /run/flannel
                  - name: flannel-cfg
                    mountPath: /etc/kube-flannel/
                  - name: cni-plugin
                    mountPath: /opt/cni/bin/
                  - name: cni
                    mountPath: /etc/cni/net.d/
                - name: install-cni
                  image: {{ registry_url }}/flannel-cni-plugin:v1.4.0
                  imagePullPolicy: IfNotPresent
                  command: ["/install-cni.sh"]
                  env:
                  - name: CNI_CONF_NAME
                    value: "10-flannel.conflist"
                  - name: CNI_NETWORK_CONFIG
                    valueFrom:
                      configMapKeyRef:
                        name: kube-flannel-cfg
                        key: cni-conf.json
                  volumeMounts:
                  - name: cni-plugin
                    mountPath: /host/opt/cni/bin
                  - name: cni
                    mountPath: /host/etc/cni/net.d
                volumes:
                - name: run
                  hostPath:
                    path: /run/flannel
                - name: cni-plugin
                  hostPath:
                    path: /opt/cni/bin
                - name: cni
                  hostPath:
                    path: /etc/cni/net.d
                - name: flannel-cfg
                  configMap:
                    name: kube-flannel-cfg
        dest: /tmp/flannel.yaml
        owner: root
        group: root
        mode: '0644'
      when: inventory_hostname == "master-node-1"

    - name: Применение манифеста Flannel
      command: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /tmp/flannel.yaml
      register: flannel_apply
      changed_when: flannel_apply.rc == 0
      when: inventory_hostname == "master-node-1"

    - name: Ожидание запуска подов Flannel
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-flannel -l app=flannel --no-headers
      register: flannel_pods
      until: flannel_pods.stdout != ""
      retries: 30
      delay: 10
      when: inventory_hostname == "master-node-1"

    - name: Проверка статуса Flannel подов
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-flannel -o wide
      register: flannel_status
      when: inventory_hostname == "master-node-1"

    - name: Отображение статуса Flannel
      debug:
        var: flannel_status.stdout
      when: inventory_hostname == "master-node-1"
  when: inventory_hostname == "master-node-1"

- name: Попытка снятия taint с control-plane узлов
  command: |
    kubectl --kubeconfig=/etc/kubernetes/admin.conf taint nodes --all node-role.kubernetes.io/control-plane- --overwrite
  register: taint_result
  failed_when: false
  changed_when: false
  when: inventory_hostname == "master-node-1"

- name: Отображение результата taint
  debug:
    msg: "Taint result: {{ taint_result.stdout }}{{ taint_result.stderr }}"
  when: inventory_hostname == "master-node-1"

- name: Принудительное создание cluster-info kubeadm
  command: |
    kubeadm init phase bootstrap-token --kubeconfig=/etc/kubernetes/admin.conf
  when: inventory_hostname == "master-node-1"

- name: Настройка RBAC для разрешения анонимного доступа к cluster-info
  command: |
    kubectl --kubeconfig=/etc/kubernetes/admin.conf create clusterrolebinding kubeadm-node-clusterinfo --clusterrole=cluster-info-reader --group=system:unauthenticated --group=system:authenticated
  failed_when: false
  changed_when: false
  when: inventory_hostname == "master-node-1"

- name: Проверка доступа к cluster-info
  command: |
    kubectl --kubeconfig=/etc/kubernetes/admin.conf get configmap cluster-info -n kube-public
  when: inventory_hostname == "master-node-1"

- name: Получение команды join для control-plane
  command: kubeadm token create --print-join-command
  register: join_command
  changed_when: false
  when: inventory_hostname == "master-node-1"

- name: Отображение полученной join команды
  debug:
    var: join_command.stdout
  when: inventory_hostname == "master-node-1"

- name: Получение certificate key с sudo
  shell: kubeadm init phase upload-certs --upload-certs
  register: cert_key
  changed_when: false
  become: true
  when: inventory_hostname == "master-node-1"

- name: Отображение полученного certificate key
  debug:
    var: cert_key.stdout
  when: inventory_hostname == "master-node-1"

- name: Извлечение certificate key из вывода
  set_fact:
    certificate_key: "{{ cert_key.stdout | regex_search('[a-f0-9]{64}') | first }}"
  when: inventory_hostname == "master-node-1"

- name: Отображение извлеченного certificate key
  debug:
    var: certificate_key
  when: inventory_hostname == "master-node-1"

- name: Сохранение команд join в переменные
  set_fact:
    control_plane_join_command: "kubeadm join {{ cluster_vip }}:6443 --token {{ join_command.stdout | regex_search('--token [^ ]+') | regex_replace('--token ') }} --discovery-token-ca-cert-hash {{ join_command.stdout | regex_search('--discovery-token-ca-cert-hash [^ ]+') | regex_replace('--discovery-token-ca-cert-hash ') }} --control-plane --certificate-key {{ certificate_key }}"
    worker_join_command: "kubeadm join {{ cluster_vip }}:6443 --token {{ join_command.stdout | regex_search('--token [^ ]+') | regex_replace('--token ') }} --discovery-token-ca-cert-hash {{ join_command.stdout | regex_search('--discovery-token-ca-cert-hash [^ ]+') | regex_replace('--discovery-token-ca-cert-hash ') }}"
  when: inventory_hostname == "master-node-1"

- name: Присоединение других мастер-узлов
  command: "{{ hostvars['master-node-1']['control_plane_join_command'] }}"
  when:
    - inventory_hostname != "master-node-1"
    - inventory_hostname in groups['masters']

- name: Настройка kubectl на других мастерах
  block:
    - name: Копирование конфигурации с первого мастера
      slurp:
        src: "{{ k8s_home }}/.kube/config"
      register: kubeconfig
      delegate_to: master-node-1

    - name: Запись конфигурации на текущий узел
      copy:
        content: "{{ kubeconfig.content | b64decode }}"
        dest: "{{ k8s_home }}/.kube/config"
        owner: "{{ k8s_user }}"
        group: "{{ k8s_group }}"
        mode: 0600
  when:
    - inventory_hostname != "master-node-1"
    - inventory_hostname in groups['masters']